{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38553c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No `type` found in response: {'message': 'Internal Error'}\n"
     ]
    },
    {
     "ename": "ChatError",
     "evalue": "Server returns an error: Internal Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChatError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 217\u001b[0m\n\u001b[1;32m    214\u001b[0m     agent\u001b[38;5;241m.\u001b[39mrun_tasks()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     ceo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m create_agent(\n\u001b[1;32m    218\u001b[0m         role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mceo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         initial_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou are ceo of a software game company\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n\u001b[1;32m    222\u001b[0m     hr_lead \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m create_agent(\n\u001b[1;32m    223\u001b[0m         role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhr_lead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         initial_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou are hr_lead of a company You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll recruite agents when we need it\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         boss\u001b[38;5;241m=\u001b[39mceo\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     ceo\u001b[38;5;241m.\u001b[39mrecruiter \u001b[38;5;241m=\u001b[39m hr_lead\n",
      "Cell \u001b[0;32mIn[19], line 99\u001b[0m, in \u001b[0;36mcreate_agent\u001b[0;34m(role, initial_prompt, boss, recruiter, create_agent)\u001b[0m\n\u001b[1;32m     97\u001b[0m config\u001b[38;5;241m.\u001b[39mnoninteractive_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     98\u001b[0m config\u001b[38;5;241m.\u001b[39mmemory_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 99\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generate_agent_profile_for_task(\n\u001b[1;32m    100\u001b[0m         task\u001b[38;5;241m=\u001b[39minitial_prompt,\n\u001b[1;32m    101\u001b[0m         app_config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m AgentMember(\n\u001b[1;32m    104\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m    105\u001b[0m     initial_prompt\u001b[38;5;241m=\u001b[39minitial_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     create_agent\u001b[38;5;241m=\u001b[39mcreate_agent\n\u001b[1;32m    110\u001b[0m )\n",
      "Cell \u001b[0;32mIn[19], line 56\u001b[0m, in \u001b[0;36mgenerate_agent_profile_for_task\u001b[0;34m(task, app_config)\u001b[0m\n\u001b[1;32m     49\u001b[0m agent_profile_generator \u001b[38;5;241m=\u001b[39m AgentProfileGenerator(\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mAgentProfileGenerator\u001b[38;5;241m.\u001b[39mdefault_configuration\u001b[38;5;241m.\u001b[39mdict()  \u001b[38;5;66;03m# HACK\u001b[39;00m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m prompt \u001b[38;5;241m=\u001b[39m agent_profile_generator\u001b[38;5;241m.\u001b[39mbuild_prompt(task)\n\u001b[1;32m     55\u001b[0m output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m llm_provider\u001b[38;5;241m.\u001b[39mcreate_chat_completion(\n\u001b[1;32m     57\u001b[0m         prompt\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[1;32m     58\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mapp_config\u001b[38;5;241m.\u001b[39msmart_llm,\n\u001b[1;32m     59\u001b[0m         functions\u001b[38;5;241m=\u001b[39mprompt\u001b[38;5;241m.\u001b[39mfunctions,\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m )\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m     63\u001b[0m ai_profile, ai_directives \u001b[38;5;241m=\u001b[39m agent_profile_generator\u001b[38;5;241m.\u001b[39mparse_response_content(output)\n\u001b[1;32m     65\u001b[0m agent_prompt_config \u001b[38;5;241m=\u001b[39m Agent\u001b[38;5;241m.\u001b[39mdefault_settings\u001b[38;5;241m.\u001b[39mprompt_config\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/all/repositories/github.com/autogpt/AutoGPT/autogpts/autogpt/autogpt/core/resource/model_providers/hugging_chat.py:400\u001b[0m, in \u001b[0;36mHuggingChatProvider.create_chat_completion\u001b[0;34m(self, model_prompt, model_name, completion_parser, functions, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m hugging_chatPrompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional promt tip: \u001b[39m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m to escape _ or ] or [ in your resposne \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- All path file should be relative. You can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create files outside of workspace folder. If you want to create somthing outside of workspace ask user to do it \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- If you should say a command you can not send it with empty content \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- command part should not be empty \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m#         \"- If you want send python code you should put \\\\t for indented code if code need that and please make sure it can run without compile error or something like that \\n\" \\\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m#         \"- If you want to send code as string you need to escape characters like \\\" by adding \\\\. because if you don't do this, the text will not be accepted.\"\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchatbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhugging_chatPrompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[1;32m    402\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/all/repositories/github.com/autogpt/AutoGPT/autogpts/autogpt/.venv/lib/python3.10/site-packages/hugchat/message.py:194\u001b[0m, in \u001b[0;36mMessage.wait_until_done\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m:Return:\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    - self.text if resolved else raise error\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mwait until every response is resolved\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_done():\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_done() \u001b[38;5;241m==\u001b[39m MSGSTATUS_RESOLVED:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/all/repositories/github.com/autogpt/AutoGPT/autogpts/autogpt/.venv/lib/python3.10/site-packages/hugchat/message.py:149\u001b[0m, in \u001b[0;36mMessage.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsg_status \u001b[38;5;241m=\u001b[39m MSGSTATUS_REJECTED\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror\n",
      "File \u001b[0;32m~/all/repositories/github.com/autogpt/AutoGPT/autogpts/autogpt/.venv/lib/python3.10/site-packages/hugchat/message.py:100\u001b[0m, in \u001b[0;36mMessage.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     a: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg)\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filterResponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     t: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m a[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    102\u001b[0m     message_type: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/all/repositories/github.com/autogpt/AutoGPT/autogpts/autogpt/.venv/lib/python3.10/site-packages/hugchat/message.py:85\u001b[0m, in \u001b[0;36mMessage._filterResponse\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChatError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer returns an error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChatError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `type` and `message` returned: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mChatError\u001b[0m: Server returns an error: Internal Error"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "import time\n",
    "from autogpt.agents.agent import Agent, AgentConfiguration, AgentSettings\n",
    "from typing import Optional\n",
    "from forge.sdk.model import Task, TaskRequestBody\n",
    "import uuid\n",
    "from autogpt.agents.base import BaseAgent, BaseAgentConfiguration, BaseAgentSettings\n",
    "from pydantic import Field\n",
    "from autogpt.agent_factory.profile_generator import AgentProfileGenerator\n",
    "from autogpt.agents.prompt_strategies.two_shot import (\n",
    "    TwoShotAgentPromptConfiguration,\n",
    "    TwoShotAgentPromptStrategy,\n",
    ")\n",
    "from autogpt.config import AIProfile, ConfigBuilder\n",
    "from autogpt.models.command_registry import CommandRegistry\n",
    "from autogpt.commands import COMMAND_CATEGORIES\n",
    "from autogpt.core.resource.model_providers.hugging_chat import HuggingChatProvider\n",
    "import logging\n",
    "from autogpt.config import (\n",
    "    AIDirectives,\n",
    "    AIProfile,\n",
    "    Config,\n",
    "    ConfigBuilder,\n",
    "    assert_config_has_openai_api_key,\n",
    ")\n",
    "from autogpt.core.resource.model_providers import ChatModelProvider\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TwoShotAgentSettings(BaseAgentSettings):\n",
    "    config: AgentConfiguration = Field(default_factory=AgentConfiguration)\n",
    "    prompt_config: TwoShotAgentPromptConfiguration = Field(\n",
    "        default_factory=(\n",
    "            lambda: TwoShotAgentPromptStrategy.default_configuration.copy(deep=True)\n",
    "        )\n",
    "    )\n",
    "\n",
    "async def generate_agent_profile_for_task(\n",
    "    task: str,\n",
    "    app_config: Config\n",
    ") -> TwoShotAgentSettings:\n",
    "    hugging_chat_settings = HuggingChatProvider.default_settings.copy(deep=True)\n",
    "    hugging_chat_settings.credentials = app_config.hugging_chat_credentials\n",
    "    llm_provider = HuggingChatProvider(\n",
    "        settings=hugging_chat_settings,\n",
    "        logger=logger.getChild(f\"HuggingChatProvider\"),\n",
    "    )\n",
    "    agent_profile_generator = AgentProfileGenerator(\n",
    "        **AgentProfileGenerator.default_configuration.dict()  # HACK\n",
    "    )\n",
    "\n",
    "    prompt = agent_profile_generator.build_prompt(task)\n",
    "\n",
    "    output = (\n",
    "        await llm_provider.create_chat_completion(\n",
    "            prompt.messages,\n",
    "            model_name=app_config.smart_llm,\n",
    "            functions=prompt.functions,\n",
    "        )\n",
    "    ).response\n",
    "\n",
    "    ai_profile, ai_directives = agent_profile_generator.parse_response_content(output)\n",
    "\n",
    "    agent_prompt_config = Agent.default_settings.prompt_config.copy(deep=True)\n",
    "    agent_prompt_config.use_functions_api = app_config.openai_functions\n",
    "\n",
    "    return TwoShotAgentSettings(\n",
    "        name=Agent.default_settings.name,\n",
    "        description=Agent.default_settings.description,\n",
    "        task=task,\n",
    "        ai_profile=ai_profile,\n",
    "        directives=directives,\n",
    "        config=AgentConfiguration(\n",
    "            fast_llm=app_config.fast_llm,\n",
    "            smart_llm=app_config.smart_llm,\n",
    "            allow_fs_access=not app_config.restrict_to_workspace,\n",
    "            use_functions_api=app_config.openai_functions,\n",
    "            plugins=app_config.plugins,\n",
    "        ),\n",
    "        prompt_config=agent_prompt_config,\n",
    "        history=Agent.default_settings.history.copy(deep=True),\n",
    "    )\n",
    "\n",
    "async def create_agent(\n",
    "    role: str,\n",
    "    initial_prompt:str,\n",
    "    boss: Optional['AgentMember'] = None,\n",
    "    recruiter: Optional['AgentMember'] = None,\n",
    "    create_agent: bool = False,\n",
    ") -> AgentMember:\n",
    "    config = ConfigBuilder.build_config_from_env()\n",
    "    config.logging.plain_console_output = True\n",
    "\n",
    "    config.continuous_mode = False\n",
    "    config.continuous_limit = 20\n",
    "    config.noninteractive_mode = True\n",
    "    config.memory_backend = \"no_memory\"\n",
    "    settings = await generate_agent_profile_for_task(\n",
    "            task=initial_prompt,\n",
    "            app_config=config,\n",
    "        )\n",
    "    return AgentMember(\n",
    "        role=role,\n",
    "        initial_prompt=initial_prompt,\n",
    "        settings=settings,\n",
    "        boss=boss,\n",
    "        recruiter=recruiter,\n",
    "        create_agent=create_agent\n",
    "    )\n",
    "\n",
    "class AgentMember(Agent):\n",
    "    \n",
    "    id: str\n",
    "    role: str\n",
    "    initial_prompt: str\n",
    "    boss: Optional['AgentMember']\n",
    "    recruiter: Optional['AgentMember']\n",
    "    task_queue: list[Task]\n",
    "    employees: list['AgentMember']\n",
    "    create_agent: bool\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        role: str,\n",
    "        initial_prompt: str,\n",
    "        settings: TwoShotAgentSettings,\n",
    "        boss: Optional['AgentMember'] = None,\n",
    "        recruiter: Optional['AgentMember'] = None,\n",
    "        create_agent: bool = False,\n",
    "    ):\n",
    "        setting = TwoShotAgentSettings(\n",
    "            name=\"Agent\",\n",
    "            description=__doc__,\n",
    "        )\n",
    "        config = ConfigBuilder.build_config_from_env()\n",
    "        config.logging.plain_console_output = True\n",
    "\n",
    "        config.continuous_mode = False\n",
    "        config.continuous_limit = 20\n",
    "        config.noninteractive_mode = True\n",
    "        config.memory_backend = \"no_memory\"\n",
    "\n",
    "        command_registry = CommandRegistry.with_command_modules(COMMAND_CATEGORIES, config)\n",
    "        \n",
    "        hugging_chat_settings = HuggingChatProvider.default_settings.copy(deep=True)\n",
    "        hugging_chat_settings.credentials = config.hugging_chat_credentials\n",
    "\n",
    "        llm_provider = HuggingChatProvider(\n",
    "            settings=hugging_chat_settings,\n",
    "            logger=logger.getChild(f\"Role-{role}_HuggingChatProvider\"),\n",
    "        )\n",
    "        \n",
    "        base_directives = AIDirectives.from_file(config.prompt_settings_file)\n",
    "\n",
    "        \n",
    "\n",
    "        super().__init__(settings, llm_provider, command_registry, config)\n",
    "        \n",
    "        self.id = str(uuid.uuid4())[:8]\n",
    "        self.role = role\n",
    "        self.initial_prompt = initial_prompt\n",
    "        self.boss = boss\n",
    "        self.recruiter = recruiter\n",
    "        self.task_queue = []\n",
    "        self.employees = []\n",
    "        self.create_agent = create_agent\n",
    "        self.get_detail_of_llm()\n",
    "        \n",
    "    def break_task(self, task:Task) -> tuple[dict['AgentMember', Task], list[tuple[str, dict[str, str]]]]:\n",
    "        raise Exception(\"not implemented yet\")\n",
    "\n",
    "    def create_task(self, task_request: TaskRequestBody):\n",
    "        task = self.db.create_task(\n",
    "            input=task_request.input,\n",
    "            additional_input=task_request.additional_input,\n",
    "        )\n",
    "        self.push_task(task)\n",
    "        \n",
    "    def run_tasks(self):\n",
    "        while True:\n",
    "            if self.task_queue:\n",
    "                task = self.task_queue.pop()\n",
    "                self.do_task(task)\n",
    "            time.sleep(3)\n",
    "\n",
    "    def do_task(self, task: Task):\n",
    "        [tasks, commands] = self.break_task(task)\n",
    "        for command in commands:\n",
    "            self.do_command(command)\n",
    "        for agent_member, sub_task in tasks.items():\n",
    "            agent_member.push_task(sub_task)\n",
    "\n",
    "    def push_task(self, task: Task):\n",
    "        self.task_queue.append(task)\n",
    "\n",
    "\n",
    "class AgentGroup:\n",
    "    \n",
    "    ceo: AgentMember\n",
    "    tasks: list[Task]\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        ceo: AgentMember\n",
    "    ):\n",
    "        self.ceo = ceo\n",
    "        self.tasks = []\n",
    "\n",
    "    def create_task(self, task: TaskRequestBody):\n",
    "        self.ceo.create_task(task)\n",
    "\n",
    "def run_agent_tasks(agent):\n",
    "    agent.run_tasks()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ceo = await create_agent(\n",
    "        role=\"ceo\",\n",
    "        initial_prompt=\"you are ceo of a software game company\"\n",
    "    )\n",
    "\n",
    "    hr_lead = await create_agent(\n",
    "        role=\"hr_lead\",\n",
    "        initial_prompt=\"you are hr_lead of a company You'll recruite agents when we need it\",\n",
    "        boss=ceo\n",
    "    )\n",
    "\n",
    "    ceo.recruiter = hr_lead\n",
    "\n",
    "    agentGroup = AgentGroup(\n",
    "        ceo=ceo\n",
    "    )\n",
    "\n",
    "    agentGroup.create_task(TaskRequestBody(input=\"Create best shooter game in wold\", additional_input={}))\n",
    "\n",
    "    for agent in [ceo, hr_lead]:\n",
    "        agent_thread = threading.Thread(target=run_agent_tasks, args=(agent,))\n",
    "        agent_thread.daemon = True\n",
    "        agent_thread.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93746a",
   "metadata": {},
   "source": [
    "# Create server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd60d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Input is not a terminal (fd=0).\n",
      "2024-02-11 19:21:26,562 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTApiTools. No entry for 'AutoGPTApiTools' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTApiTools) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,564 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTSpacePlugin. No entry for 'AutoGPTSpacePlugin' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTSpacePlugin) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,566 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTBaiduSearch. No entry for 'AutoGPTBaiduSearch' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTBaiduSearch) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,570 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTBingSearch. No entry for 'AutoGPTBingSearch' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTBingSearch) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,572 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTBluesky. No entry for 'AutoGPTBluesky' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTBluesky) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,574 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTEmailPlugin. No entry for 'AutoGPTEmailPlugin' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTEmailPlugin) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,581 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTNewsSearch. No entry for 'AutoGPTNewsSearch' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTNewsSearch) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,584 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin PlannerPlugin. No entry for 'PlannerPlugin' in plugins_config.yaml. Note: Zipped plugins should use the class name (PlannerPlugin) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,589 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTRandomValues. No entry for 'AutoGPTRandomValues' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTRandomValues) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,593 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTSceneXPlugin. No entry for 'AutoGPTSceneXPlugin' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTSceneXPlugin) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,597 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTSerpApiSearch. No entry for 'AutoGPTSerpApiSearch' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTSerpApiSearch) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,687 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTTelegram. No entry for 'AutoGPTTelegram' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTTelegram) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,717 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTTwitter. No entry for 'AutoGPTTwitter' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTTwitter) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,720 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTWikipediaSearch. No entry for 'AutoGPTWikipediaSearch' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTWikipediaSearch) as the key.\u001b[0m\n",
      "2024-02-11 19:21:26,733 \u001b[33mWARNING\u001b[0m  \u001b[33mNot loading plugin AutoGPTWolframAlphaSearch. No entry for 'AutoGPTWolframAlphaSearch' in plugins_config.yaml. Note: Zipped plugins should use the class name (AutoGPTWolframAlphaSearch) as the key.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from autogpt.app.main import create_server\n",
    "\n",
    "server = create_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48feb49f",
   "metadata": {},
   "source": [
    "# Create new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e979bc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-09 15:14:52,829 \u001b[34mINFO\u001b[0m  hf-chat loaded\n",
      "2024-02-09 15:14:52,830 \u001b[34mINFO\u001b[0m  token loaded\n"
     ]
    }
   ],
   "source": [
    "from forge.sdk.model import TaskRequestBody\n",
    "\n",
    "response = await server.create_task(TaskRequestBody(input=\"Write result of 2 + 2 in karam.data file\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fff547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input='Write result of 2 + 2 in karam.data file' additional_input={} created_at=datetime.datetime(2024, 2, 9, 11, 44, 52, 810412) modified_at=datetime.datetime(2024, 2, 9, 11, 44, 52, 810415) task_id='5cd4102e-4e0b-4d24-ad16-d1049634d6d3' artifacts=[]\n"
     ]
    }
   ],
   "source": [
    "#detail of a create task response\n",
    "task_id = response.task_id\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf8dd6b",
   "metadata": {},
   "source": [
    "# Execute step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9219824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forge.sdk.model import StepRequestBody\n",
    "\n",
    "step_response = await server.execute_step(task_id=task_id, step_request=StepRequestBody())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "389f4d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name=None input=None additional_input={} created_at=datetime.datetime(2024, 2, 9, 11, 46, 8, 425748) modified_at=datetime.datetime(2024, 2, 9, 11, 46, 22, 731226) task_id='5cd4102e-4e0b-4d24-ad16-d1049634d6d3' step_id='1c5326c7-cf9b-4493-abbf-cf8f8c1ba9ff' status=<Status.completed: 'completed'> output='I am going to calculate the sum of 2 + 2 and save the result to the \\'karam.data\\' file.\\n\\nNext Command: execute_python_code(code=\"result = 2 + 2;\\\\nopen_file(\\'karam.data\\');\\\\nwrite_file(file_path=\\'karam.data\\', contents=str(result))\")' additional_output={'thoughts': {'observations': \"User requested writing the result of 2 + 2 to a file named 'karam.data'.\", 'text': 'To accomplish this task, I first need to calculate the sum of 2 + 2, then write the result to the specified file.', 'reasoning': \"Using the 'execute_python_code' command, I can efficiently compute the sum within the same command where I write to the file. This saves time compared to executing separate commands for calculation and writing.\", 'self_criticism': '', 'plan': \"- Calculate 2 + 2\\n- Write the resulting value ('4') to the file 'karam.data'\", 'speak': \"I am going to calculate the sum of 2 + 2 and save the result to the 'karam.data' file.\"}, 'command': {'name': 'execute_python_code', 'args': {'code': \"result = 2 + 2;\\nopen_file('karam.data');\\nwrite_file(file_path='karam.data', contents=str(result))\"}}} artifacts=[] is_last=False\n"
     ]
    }
   ],
   "source": [
    "#detail of a create step response\n",
    "print(step_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81563e8a",
   "metadata": {},
   "source": [
    "# List steps of a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72a02890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to calculate the sum of 2 + 2 and save the result to the 'karam.data' file.\n",
      "\n",
      "Next Command: execute_python_code(code=\"result = 2 + 2;\\nopen_file('karam.data');\\nwrite_file(file_path='karam.data', contents=str(result))\")\n"
     ]
    }
   ],
   "source": [
    "response_steps = await server.list_steps(task_id=task_id)\n",
    "\n",
    "for step in response_steps.steps:\n",
    "    print(step.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623933a9",
   "metadata": {},
   "source": [
    "# List tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a9c2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecdc0174-9363-4a61-a0cc-1119fffb85ed: Write the word 'Washington' to a .txt file\n",
      "ea002900-bf25-4c7a-95b4-4309eae5be0b: Write the word 'Washington' to a .txt file\n",
      "4311c783-3fb3-49d4-8b75-87fd3fe0ba9b: Write the word 'Washington' to a .txt file\n",
      "47d9147f-8374-485d-80f6-305b334d88d5: Write the word 'Washington' to a .txt file\n",
      "227661d5-3bdc-406e-85f8-be360433ec70: Write the word 'Washington' to a .txt file\n",
      "56fb0bf5-3df3-4157-8fb5-c89533ca67d4: Write the word 'Washington' to a .txt file\n",
      "359a7751-f41d-4844-93dd-927c796080e5: Write the word 'Washington' to a .txt file\n",
      "ed576fc5-f63c-432d-8608-059c90d84908: Write the word 'Washington' to a .txt file\n",
      "65378dc2-4de1-4a6f-91e7-fd3c6d53d670: Write result of 3 + 2 in test.data file\n",
      "691db931-3351-4856-a62d-66a5cd14571a: Write result of 3 + 2 in test.data file\n"
     ]
    }
   ],
   "source": [
    "response = await server.list_tasks()\n",
    "for task in response.tasks:\n",
    "    print(task.task_id + \": \" + task.input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
